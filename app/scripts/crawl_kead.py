import requests
from bs4 import BeautifulSoup
import json

# í¬ë¡¤ë§í•  í˜ì´ì§€ë“¤
URLS = [
    "https://www.kead.or.kr/edsysdesc/cntntsPage.do?menuId=MENU0684",
    "https://www.kead.or.kr/edhowapply/cntntsPage.do?menuId=MENU0686",
    "https://www.kead.or.kr/edclcmth/cntntsPage.do?menuId=MENU0687",
    "https://www.kead.or.kr/edfrdlbnft/cntntsPage.do?menuId=MENU0688",
    "https://www.kead.or.kr/edformlbrry/cntntsPage.do?menuId=MENU0689",
    "https://www.kead.or.kr/bbs/faq/bbsPage.do?adt1Code=002&menuId=MENU0691",
    "https://www.kead.or.kr/bbs/faq/bbsPage.do?pageIndex=2&bbsCode=faq&bbsCnId=&bbsNm=%EC%A7%88%EB%AC%B8%EA%B3%BC+%EB%8B%B5%EB%B3%80&menuId=MENU0691&adt1Code=002&adt2Code=&searchCondition=sjcn&searchKeyword=&recordCountPerPage=10",
    "https://www.kead.or.kr/bbs/faq/bbsPage.do?pageIndex=3&bbsCode=faq&bbsCnId=&bbsNm=%EC%A7%88%EB%AC%B8%EA%B3%BC+%EB%8B%B5%EB%B3%80&menuId=MENU0691&adt1Code=002&adt2Code=&searchCondition=sjcn&searchKeyword=&recordCountPerPage=10",
    "https://www.kead.or.kr/nptsysdesc/cntntsPage.do?menuId=MENU0899",
    "https://www.kead.or.kr/ndthowapply/cntntsPage.do?menuId=MENU0900",
    "https://www.kead.or.kr/emplymncst/cntntsPage.do?menuId=MENU0666",
    "https://www.kead.or.kr/ffssysdesc/cntntsPage.do?menuId=MENU0701",
    "https://www.kead.or.kr/bbs/faq/bbsPage.do?adt1Code=007&menuId=MENU0702",
    "https://www.kead.or.kr/emplyobsys/cntntsPage.do?menuId=MENU0648",
    "https://www.kead.or.kr/spremoblgt/cntntsPage.do?menuId=MENU0649",
    "https://www.kead.or.kr/epstempemppln/cntntsPage.do?menuId=MENU0651",
    "https://www.kead.or.kr/epstcvlstempln/cntntsPage.do?menuId=MENU0652",
    "https://www.kead.or.kr/emplfsysdsc/cntntsPage.do?menuId=MENU0654",
    "https://www.kead.or.kr/emplfeycpcds/cntntsPage.do?menuId=MENU0655",
    "https://www.kead.or.kr/bbs/faq/bbsPage.do?adt1Code=011&menuId=MENU0656"
]

# ğŸ”¹ í‘œ ë‚´ìš© ì¶”ì¶œ
def extract_table_texts(content_area):
    tables = content_area.find_all("table")
    table_texts = []

    for table in tables:
        rows = table.find_all("tr")
        for row in rows:
            cols = [col.get_text(strip=True) for col in row.find_all(["td", "th"])]
            if cols:
                table_texts.append(" | ".join(cols))

    return table_texts

# ğŸ”¹ FAQ (ì•„ì½”ë””ì–¸ í¬í•¨) ì¶”ì¶œ í†µí•© í•¨ìˆ˜
def extract_accordion_faqs(soup):
    faq_lines = []

    # âœ… ê¸°ì¡´ êµ¬ì¡° (ul.faq_list, dl.faq)
    faq_items = soup.select("ul.faq_list li") or soup.select("dl.faq dt")
    for item in faq_items:
        question = item.select_one("a, dt")
        answer = item.find_next_sibling("dd") or item.select_one("div.answer, dd")
        if question and answer:
            q = question.get_text(strip=True)
            a = answer.get_text(strip=True)
            faq_lines.append(f"Q: {q}\nA: {a}")

    # âœ… ì¶”ê°€: board_qa ì•„ì½”ë””ì–¸ êµ¬ì¡°
    board_qa = soup.select("dl.board_qa dt")
    for dt in board_qa:
        button = dt.select_one("button")
        dd = dt.find_next_sibling("dd")
        if button and dd:
            q = button.get_text(strip=True).replace("Q", "").strip()
            a = dd.get_text(strip=True)
            faq_lines.append(f"Q: {q}\nA: {a}")

    return faq_lines

# ğŸ”¹ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
def classify_category(title: str, body: str, url: str) -> str:
    lower_title = title.lower()
    lower_body = body.lower()

    # âœ… FAQ ìš°ì„  ì²˜ë¦¬
    if "faq" in url or "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸" in body or "Q:" in body:
        return "FAQ"

    # âœ… ì§€ì›ê¸ˆ/ì§€ì›ì œë„ ë¨¼ì € ì²´í¬
    if any(keyword in lower_title + lower_body for keyword in ["ì¥ë ¤ê¸ˆ", "ì§€ì›ê¸ˆ", "ì§€ê¸‰ë‹¨ê°€", "ì„ê¸ˆì§€ì›", "ì‹ ê·œê³ ìš©"]):
        return "ì§€ì›ê¸ˆ"
    if any(keyword in lower_title + lower_body for keyword in ["ë¬´ìƒì§€ì›", "ê·¼ë¡œì§€ì›ë¹„ìš©", "ì‘ì—…ì¥ë¹„", "í¸ì˜ì‹œì„¤", "ì‘ì—…ì§€ë„ì›", "ì¬íƒê·¼ë¬´"]):
        return "ì§€ì›ì œë„"

    # âœ… ì˜ë¬´ì œë„ (ì§€ì›ì´ ì—†ê³  ì˜ë¬´ë§Œ ê°•ì¡°ëœ ê²½ìš°ë§Œ í•´ë‹¹)
    if any(keyword in lower_body for keyword in ["ì˜ë¬´ê³ ìš©ë¥ ", "ì˜ë¬´ê³ ìš©", "ê³ ìš©ì˜ë¬´", "ê³ ìš©ë¥  ì´ˆê³¼", "ê³ ìš©ë¥  ë¯¸ë‹¬"]):
        return "ì˜ë¬´ì œë„"

    # âœ… ë¶€ë‹´ê¸ˆ
    if any(keyword in lower_body for keyword in ["ë¶€ë‹´ê¸ˆ", "ì§•ìˆ˜", "í™˜ìˆ˜", "ì œì¬", "ë²Œì¹™"]):
        return "ë¶€ë‹´ê¸ˆ"

    return "ê¸°íƒ€"


# ğŸ”¹ í•œ í˜ì´ì§€ ì²˜ë¦¬
def fetch_policy(url: str):
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    title = soup.title.string.strip()

    # ğŸ“Œ FAQ ì „ìš© í˜ì´ì§€ ì²˜ë¦¬ ë¶„ê¸°
    if "bbs/faq" in url:
        faq_lines = extract_accordion_faqs(soup)
        if not faq_lines:
            raise ValueError(f"[âŒ] FAQ ë‚´ìš© ì—†ìŒ: {url}")
        body = "\n[ìì£¼ ë¬»ëŠ” ì§ˆë¬¸]\n" + "\n".join(faq_lines)
        category = classify_category(title, body, url)
        return {
            "title": title,
            "body": body,
            "category": category,
            "url": url,
            "embedding": None
        }

    # ğŸ“Œ ì¼ë°˜ ì½˜í…ì¸  í˜ì´ì§€
    content_area = (
        soup.select_one("main#main_content article.content_width div.body.text") or
        soup.find("div", id="body_text") or
        soup.find("div", class_="body_text")
    )
    if not content_area:
        raise ValueError(f"[âŒ] ë³¸ë¬¸ ì˜ì—­ ì—†ìŒ: {url}")

    # ë³¸ë¬¸ ì¶”ì¶œ (data-brl-use ìš°ì„ )
    brl_elements = content_area.find_all(attrs={"data-brl-use": True})
    lines = [el.get_text(strip=True) for el in brl_elements if el.get_text(strip=True)]

    if not lines:
        lines = [tag.get_text(strip=True) for tag in content_area.find_all(["p", "div", "li", "span", "h4"]) if tag.get_text(strip=True)]

    # í‘œ
    table_lines = extract_table_texts(content_area)
    if table_lines:
        lines.append("\n[í‘œ ì •ë³´]")
        lines.extend(table_lines)

    # FAQ
    faq_lines = extract_accordion_faqs(soup)
    if faq_lines:
        lines.append("\n[ìì£¼ ë¬»ëŠ” ì§ˆë¬¸]")
        lines.extend(faq_lines)

    # ë³¸ë¬¸ êµ¬ì„±
    body = "\n".join(lines)
    category = classify_category(title, body, url)

    return {
        "title": title,
        "body": body,
        "category": category,
        "url": url,
        "embedding": None
    }

# ğŸ”¹ ì‹¤í–‰
if __name__ == "__main__":
    all_docs = []

    for url in URLS:
        try:
            doc = fetch_policy(url)
            all_docs.append(doc)
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {url} ({len(doc['body'])}ì)")
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {url} â†’ {e}")

    with open("scripts/policies.json", "w", encoding="utf-8") as f:
        json.dump(all_docs, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“¦ ì´ {len(all_docs)}ê±´ ì €ì¥ë¨ â†’ scripts/policies.json")